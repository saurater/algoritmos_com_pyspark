{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. PySpark RDD - Lower - Split - Len.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMGosYHZ5xx+gbF6xR1W/8s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saurater/algoritmos_com_pyspark/blob/main/1_PySpark_RDD_Lower_Split_Len.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algoritmos com PySpark - 1. Pyspark RDD - Lower - Split - Len\n",
        "\n",
        "Sam Faraday\n",
        "\n",
        "Baseado no livro PySpark Algorithms - Marmoud Parsian"
      ],
      "metadata": {
        "id": "gvuIMCQyGQgW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalamos o pyspark \n",
        "no Jupityer vc faz isso apenas uma vez\n",
        "\n",
        "No Colab preica fazer isso toda vez que iniciar a sessão"
      ],
      "metadata": {
        "id": "b-DSCbu9C3_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "id": "6yu9-6Yd0eXv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03ab784c-5ff6-4391-d262-05604d4b2cd6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Requirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importamos o SparkSession para configuramos e iniciarmos a Sessão Spark"
      ],
      "metadata": {
        "id": "J-C16P1sDJCc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Q0wrsaPf0dQg"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark  =  SparkSession.builder.appName(\"Trabalho_Pratico\").getOrCreate()"
      ],
      "metadata": {
        "id": "YwcLELxL0f_W"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criamos uma arquivo txt para nossos testes"
      ],
      "metadata": {
        "id": "ktsqrjPqDQQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arquivo_exemplo_txt= open(\"arquivo_exemplo_txt\",\"w+\")"
      ],
      "metadata": {
        "id": "kPEWiXtv1Gex"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adicionamos a primeira linha"
      ],
      "metadata": {
        "id": "LKiVGQffDX_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arquivo_exemplo_txt.write(\"Red,Fox,is,Fast\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pU8fPs-1Xp1",
        "outputId": "a3b2deb8-9f96-4fe1-c812-56a8bd06c3f4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adicionamos a segunda linha"
      ],
      "metadata": {
        "id": "EgvsyRnIDceB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arquivo_exemplo_txt.write(\"Red,Fox,by,Here\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FUE1NuR14DZ",
        "outputId": "672f2d8f-1270-44e3-944e-4e28e0ae758e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fechamos o Arquivo txt"
      ],
      "metadata": {
        "id": "gOB2efDxDikZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arquivo_exemplo_txt.close()"
      ],
      "metadata": {
        "id": "hqNGc8FY2IjV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SPARK - Criamos um objeto RDD (Resilient Distributed Data)\n",
        "fazemos isso carregando o arquivo para a memória do Spark"
      ],
      "metadata": {
        "id": "HB-j8FmgDoR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_arquivo_exemplo_txt = spark.sparkContext.textFile(\"arquivo_exemplo_txt\")"
      ],
      "metadata": {
        "id": "4xgwMeh22sP8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imprimimos as 10 primeiras linhas"
      ],
      "metadata": {
        "id": "yjDCvJ8gD6KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (rdd_arquivo_exemplo_txt.take(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w89CRgJi3k5n",
        "outputId": "8dcab148-447e-4fd2-ae44-333a596b828a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Red,Fox,is,Fast', 'Red,Fox,by,Here']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Aplicamos a primeira transformação, lower(), com um map e uma função lambda\n",
        "lemos linha a linha\n",
        "\n",
        "Cada linha é transformada em \"letras_minúsculas\" com as funções map e lower\n",
        "\n",
        "**map** aplica a função **lambda** em cada linha"
      ],
      "metadata": {
        "id": "1wUUBqoUAQm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_arquivo_exemplo_txt = rdd_arquivo_exemplo_txt.map(lambda linha : linha.lower())"
      ],
      "metadata": {
        "id": "8bVz5bDp4Rkb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imprimimos com take(10) que pega apenas as 10 primeiras ocorrências"
      ],
      "metadata": {
        "id": "iWNOSOJKBNae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (rdd_arquivo_exemplo_txt.take(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHKiA6jc6GHS",
        "outputId": "ce73f84d-579f-4f09-a83c-63f11935b73f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['red,fox,is,fast', 'red,fox,by,here']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Aplicamos 2 transformações em sequência: lower com map e split com flatMap usando uma função lambda\n",
        "\n",
        "lemos linha a linha\n",
        "\n",
        "Cada linha é transformada em \"letras_minúsculas\" com as funções map e lower\n",
        "\n",
        "Depois fazemos um split (separação) de cada palavra com flatMap\n",
        "\n",
        "**map** atua por linha\n",
        "\n",
        "**flatMap** atua por palavra\n"
      ],
      "metadata": {
        "id": "f4Y8t-teBvhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#estamos lendo o arquivo orignal de novo apenas para fins didáticos\n",
        "#na prática, faríamos tudo com uma única linha, no exemplo 3\n",
        "rdd_arquivo_exemplo_txt = spark.sparkContext.textFile(\"arquivo_exemplo_txt\")"
      ],
      "metadata": {
        "id": "csxDP0VJE3yT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_arquivo_exemplo_txt = rdd_arquivo_exemplo_txt.map(lambda linha : linha.lower()).flatMap(lambda linha: linha.split(','))"
      ],
      "metadata": {
        "id": "YxgKrji26VBB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (rdd_arquivo_exemplo_txt.take(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTeLNvtl6fii",
        "outputId": "e3b2eac0-b5b6-457f-b790-c6f6cd944ae9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['red', 'fox', 'is', 'fast', 'red', 'fox', 'by', 'here']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Aplicamos 3 transformações em sequência:\n",
        "\n",
        "## lower com um map \n",
        "## split com flatMap\n",
        "## filter com flatMap - aplicamos um filtro, pegando apenas palavras com comprimento (len) > 2 usando uma função lambda\n",
        "\n",
        "lemos linha a linha\n",
        "\n",
        "Cada linha é transformada em \"letras_minúsculas com as funções map e lower\n",
        "\n",
        "Depois fazemos um split (separação) de cada palavra com flatMap\n",
        "\n",
        "Depois filter das palavras com mais de 2 caracteres (len > 2) com flatMap\n",
        "\n",
        "**map** atua por linha\n",
        "\n",
        "**flatMap** atua por palavra\n"
      ],
      "metadata": {
        "id": "4S7f3HqLCtU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#estamos lendo o arquivo orignal de novo apenas para fins didáticos\n",
        "#na prática, faríamos tudo com uma única linha, nesse exemplo 3\n",
        "rdd_arquivo_exemplo_txt = spark.sparkContext.textFile(\"arquivo_exemplo_txt\")"
      ],
      "metadata": {
        "id": "f9lzNk5mFKZl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_arquivo_exemplo_txt = rdd_arquivo_exemplo_txt.map(lambda linha : linha.lower()).flatMap(lambda linha: linha.split(',')).filter(lambda linha: len(linha) >2)"
      ],
      "metadata": {
        "id": "CtI2Pshz6tGM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (rdd_arquivo_exemplo_txt.take(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SFXH9_B60Li",
        "outputId": "4389e582-23de-449f-efc9-4132194053c6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['red', 'fox', 'fast', 'red', 'fox', 'here']\n"
          ]
        }
      ]
    }
  ]
}